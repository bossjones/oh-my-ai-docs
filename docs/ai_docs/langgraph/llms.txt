[Concepts](https://langchain-ai.github.io/langgraph/concepts/): LLM should read this page when seeking to understand LangGraph concepts, exploring platform deployment options, or learning about graph-based AI workflows. (This page provides a conceptual guide to LangGraph, covering topics like graph primitives, agentic patterns, multi-agent systems, breakpoints, human-in-the-loop, time travel, persistence, memory, streaming, and the LangGraph Platform with its various deployment options and components.)

[Agent architectures](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/): LLM should read this page when working with agent implementation patterns, designing custom agent architectures, or understanding different types of LLM control flow. (This page covers LangGraph agent architectures, explaining routers with structured outputs, tool-calling agents implementing ReAct pattern (tool calling, memory, planning), and custom architectures including human-in-the-loop, parallelization, subgraphs, and reflection mechanisms.)

[Application Structure](https://langchain-ai.github.io/langgraph/concepts/application_structure/): LLM should read this page when creating a LangGraph application, deploying to LangGraph Platform, or understanding proper application structure. (This page explains LangGraph application structure, including: key concepts, file organization patterns, configuration with langgraph.json, dependency management, graph definition, and environment variable handling for both Python and JavaScript implementations.)

[Assistants](https://langchain-ai.github.io/langgraph/concepts/assistants/): LLM should read this page when looking for information about LangGraph assistants, needing to configure agent parameters, or managing assistant versions. (This page explains Assistants in LangGraph Platform, which allow developers to configure and version agents without changing graph logic. It covers how to create, configure, and version assistants for different use cases while sharing the same graph structure but with different models and prompts.)

[Authentication & Access Control](https://langchain-ai.github.io/langgraph/concepts/auth/): LLM should read this page when (seeking to implement authentication and authorization in LangGraph Platform, configuring access control for LangGraph resources, or securing a LangGraph server deployment) (Comprehensive guide to authentication and authorization in LangGraph Platform, covering core security concepts, system architecture, implementing auth handlers, resource filtering operations, common access patterns, and supported resource types with their action handlers)

[Breakpoints](https://langchain-ai.github.io/langgraph/concepts/breakpoints/): LLM should read this page when needing to implement debugging pauses in LangGraph workflows, understanding how to create human-in-the-loop systems, or troubleshooting graph execution. (Explains breakpoints in LangGraph which pause execution at specific points to enable debugging or human intervention. Covers static breakpoints that trigger before/after nodes, NodeInterrupt exceptions for conditional pausing, and requirements for implementation including checkpointers and thread IDs.)

[Bring Your Own Cloud (BYOC)](https://langchain-ai.github.io/langgraph/concepts/bring_your_own_cloud/): LLM should read this page when helping users implement Bring Your Own Cloud (BYOC) with LangGraph, explaining LangGraph's cloud deployment architecture, or addressing data security concerns with cloud services. (Describes LangGraph's BYOC offering that separates control plane (managed by LangChain) from data plane (hosted in customer's AWS account), including architecture details, requirements, implementation steps using Terraform, and security considerations for keeping customer data within their own cloud environment.)

[Deployment Options](https://langchain-ai.github.io/langgraph/concepts/deployment_options/): LLM should read this page when looking for LangGraph Platform deployment options, comparing different deployment models, or evaluating hosting choices for LangGraph. (Overview of LangGraph Platform's four deployment options: Cloud SaaS for fully managed deployment, Self-Hosted Data Plane for managing your own data infrastructure, Self-Hosted Control Plane for complete self-hosting, and Standalone Container for maximum deployment flexibility; includes availability by plan type and links to detailed implementation guides.)

[Double Texting](https://langchain-ai.github.io/langgraph/concepts/double_texting/): LLM should read this page when needing to understand double texting options in LangGraph Platform, dealing with concurrent user inputs, or implementing messaging flow controls. (This page explains how LangGraph Platform handles "double texting" - when a user sends additional inputs before the first run completes - with four approaches: Reject, Enqueue, Interrupt, and Rollback. Each approach handles concurrent inputs differently with trade-offs in preserving state and user experience.)

[Durable Execution](https://langchain-ai.github.io/langgraph/concepts/durable_execution/): LLM should read this page when needing to implement pause-and-resume capabilities in workflows, handling long-running tasks with potential interruptions, or implementing human-in-the-loop processes. (Explains durable execution in LangGraph, which enables workflows to save progress and resume from the same point after interruptions. Covers requirements (persistence, thread identifiers, tasks), determinism considerations, using tasks in nodes to handle side effects, resuming workflows after pauses or failures, and identifying proper starting points for resumption.)

[FAQ](https://langchain-ai.github.io/langgraph/concepts/faq/): LLM should read this page when needing to understand differences between LangGraph and LangChain, or when deciding deployment options, or when determining LangGraph compatibility with different LLMs. (FAQ page covering LangGraph essentials: differences from LangChain, comparison to other agent frameworks, LangGraph/Platform distinctions, deployment options, LLM compatibility including non-tool-calling and OSS LLMs, performance impact, open-source status, and LangGraph Studio usage without LangSmith)

[Functional API](https://langchain-ai.github.io/langgraph/concepts/functional_api/): LLM should read this page when (needing to understand LangGraph's Functional API, implementing human-in-the-loop workflows, or integrating persistence features into existing code) (A detailed guide to LangGraph's Functional API, explaining how to use @entrypoint and @task decorators to add persistence, human-in-the-loop interactions, memory, and streaming with minimal changes to existing code; includes examples, common patterns, and best practices for handling state management and ensuring determinism)

[Why LangGraph?](https://langchain-ai.github.io/langgraph/concepts/high_level/): LLM should read this page when exploring agent workflows, understanding LangGraph's core benefits, or evaluating infrastructure for LLM applications. (This page explains why LangGraph is valuable, highlighting its three main benefits: persistence for memory and human-in-the-loop capabilities, streaming support for real-time updates, and debugging/deployment tools including LangGraph Platform and Studio.)

[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/): LLM should read this page when (needing to implement human-in-the-loop workflows with LangGraph, designing agent systems that require human approval or input, or troubleshooting interrupt-based interactions) (Comprehensive guide to human-in-the-loop patterns in LangGraph using the interrupt function, covering approval/rejection workflows, state editing, tool call review, multi-turn conversations, and input validation, with detailed explanations of the Command primitive and common pitfalls)

[LangGraph CLI](https://langchain-ai.github.io/langgraph/concepts/langgraph_cli/): LLM should read this page when needing to use LangGraph CLI tools, deploying LangGraph servers locally, or building Docker images for LangGraph applications. (The LangGraph CLI is a command-line tool for managing LangGraph API servers with four main commands: 'build' for creating Docker images, 'dev' for lightweight development servers with hot reloading, 'up' for running local Docker instances, and 'dockerfile' for generating custom Dockerfiles.)

[Cloud SaaS (Beta)](https://langchain-ai.github.io/langgraph/concepts/langgraph_cloud/): LLM should read this page when looking for information about LangGraph's Cloud SaaS deployment option, when exploring managed LangGraph hosting solutions, or when comparing LangGraph deployment architectures. (This page describes LangGraph's Cloud SaaS deployment option (in Beta), which is a fully managed solution where LangChain handles both the control plane and data plane in their cloud. It explains the architecture, components, and management responsibilities compared to other deployment options.)

[LangGraph Control Plane](https://langchain-ai.github.io/langgraph/concepts/langgraph_control_plane/): LLM should read this page when seeking information about LangGraph's control plane functionality, when understanding deployment management in LangGraph, or when exploring LangGraph's platform architecture. (The LangGraph Control Plane is both a UI and API system for managing LangGraph Server deployments, featuring deployment types (Development/Production), database provisioning, asynchronous deployment, automatic deletion after non-use, and LangSmith integration for tracing applications.)

[](https://langchain-ai.github.io/langgraph/concepts/langgraph_control_plane/&): LLM should read this page when (looking for information about LangGraph but encountering a 404 error, trying to navigate LangGraph documentation, needing to understand why certain LangGraph pages aren't available) (This is a 404 error page from the LangGraph documentation site, indicating the requested page was not found. The page includes navigation links to LangGraph main page, Agents section, and API reference.)

[LangGraph Data Plane](https://langchain-ai.github.io/langgraph/concepts/langgraph_data_plane/): LLM should read this page when looking for information about LangGraph's data plane infrastructure, when troubleshooting deployment issues, or when evaluating LangGraph deployment options. (This page describes the LangGraph Data Plane, which includes server infrastructure, the "listener" application that polls for updates, and features like autoscaling, custom databases, IP addressing, tracing, telemetry, and licensing differences between Lite and Enterprise versions.)

[LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/): LLM should read this page when needing to understand what LangGraph Platform offers, when evaluating deployment options for agentic applications, or when seeking solutions for production-level LLM application challenges. (The page explains LangGraph Platform, a commercial solution for deploying agentic applications, detailing its components including LangGraph Server, Studio, CLI, SDK, and Control/Data Planes. It highlights key benefits such as streaming support, background runs, long-run handling, burstiness management, double texting solutions, checkpointers for persistence, and human-in-the-loop capabilities.)

[Self-Hosted Control Plane (Beta)](https://langchain-ai.github.io/langgraph/concepts/langgraph_self_hosted_control_plane/): LLM should read this page when needing information about self-hosted LangGraph control plane deployment, understanding deployment architecture options, or exploring Kubernetes-based LangGraph hosting. (This page describes the Self-Hosted Control Plane deployment option for LangGraph, where users manage both control plane and data plane in their own cloud environment. It covers the architecture, responsibilities, and specifically mentions Kubernetes as a supported compute platform.)

[Self-Hosted Data Plane (Beta)](https://langchain-ai.github.io/langgraph/concepts/langgraph_self_hosted_data_plane/): LLM should read this page when seeking information about LangGraph's self-hosted data plane deployment option, understanding hybrid cloud deployment architectures, or exploring Kubernetes-based LangGraph deployments. (Describes the Self-Hosted Data Plane deployment option for LangGraph Platform, a hybrid model where the control plane is managed by LangChain in their cloud while you manage the data plane in your own cloud environment. Includes architectural overview, comparison of responsibilities, and currently supported compute platforms like Kubernetes.)

[LangGraph Server](https://langchain-ai.github.io/langgraph/concepts/langgraph_server/): LLM should read this page when needing to understand LangGraph Server functionality, when deploying agent-based applications, or when learning about the assistants API model. (Overview of LangGraph Server, a component for deploying agent applications with built-in persistence, task queues, and streaming endpoints, featuring assistants/threads/runs architecture, support for background processing, webhooks, cron jobs, and horizontal scaling capabilities)

[Standalone Container](https://langchain-ai.github.io/langgraph/concepts/langgraph_standalone_container/): LLM should read this page when needing information about the Standalone Container deployment option for LangGraph, when comparing different LangGraph deployment architectures, or when planning to self-host LangGraph infrastructure. (The page describes the Standalone Container deployment option for LangGraph Server, which is the least restrictive deployment model where users manage their own data plane infrastructure without a control plane. It includes architecture diagrams and supports deployment on Kubernetes or any Docker-supported compute platform.)

[LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/): LLM should read this page when looking for information about LangGraph Studio, understanding how to debug agents visually, or learning about agent development environments. (LangGraph Studio is a specialized IDE for LLM applications that enables visualization, interaction, and debugging of agent workflows with features including graph visualization, state modification, testing capabilities, assistant management, thread management, memory management, and integration with LangSmith for data collection. The page explains how to access Studio through deployed applications or local development servers, provides troubleshooting for common issues like extra graph edges, and answers FAQs about interrupts, project startup problems, and automatic rebuilding.)

[LangGraph Glossary](https://langchain-ai.github.io/langgraph/concepts/low_level/): LLM should read this page when needing to understand LangGraph's core concepts, when implementing or debugging agent workflows, or when designing complex multi-agent systems. (Comprehensive glossary explaining LangGraph concepts including StateGraph, nodes, edges, state management, messaging, command flows, subgraphs, persistence mechanisms, human-in-the-loop interactions, and visualization capabilities)

[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/): LLM should read this page when needing to understand memory implementation in LangGraph agents, designing conversational agents that remember past interactions, or implementing long-term memory across conversations. (This page explains memory concepts in LangGraph, covering both short-term memory (thread-scoped, managed through state) and long-term memory (across threads, using stores). It details techniques for managing conversation history, different memory types (semantic, episodic, procedural), and approaches to writing memories (hot path vs. background).)

[Multi-agent Systems](https://langchain-ai.github.io/langgraph/concepts/multi_agent/): LLM should read this page when (seeking to build multi-agent systems, needing to implement communication between multiple agents, or comparing different agent architecture patterns) (Comprehensive guide to multi-agent systems in LangGraph including architectures (handoffs, network, supervisor, hierarchical), communication patterns between agents, state management, and implementation examples with code samples)

[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/): LLM should read this page when (implementing persistence in LangGraph applications; developing agents with memory; implementing human-in-the-loop systems) (This page explains LangGraph's persistence layer, which saves checkpoints of graph state during execution. Key topics include: threads (unique IDs for checkpoints), checkpoint management (get_state, get_state_history, replay, update_state), Memory Store for sharing information across threads, semantic search capabilities, checkpointer libraries, and how persistence enables human-in-the-loop workflows, memory between interactions, time travel for debugging, and fault tolerance.)

[LangGraph Platform Plans](https://langchain-ai.github.io/langgraph/concepts/plans/): LLM should read this page when needing to understand LangGraph Platform pricing tiers, when advising on deployment options, or when comparing features across different plans. (Overview of LangGraph Platform's three plans: Developer (free), Plus (in beta), and Enterprise (custom pricing), with detailed feature comparison and deployment options for each tier.)

[](https://langchain-ai.github.io/langgraph/concepts/plans/&): LLM should read this page when (looking for LangGraph documentation but encountered a 404 error, trying to navigate LangGraph resources, seeking information about Interrupt conference) (This is a 404 error page for LangGraph documentation, showing navigation options to LangGraph main page, Agents section, and API reference, plus a promotion for the Interrupt Agent AI Conference)

[LangGraph Platform Architecture](https://langchain-ai.github.io/langgraph/concepts/platform_architecture/): LLM should read this page when needing to understand LangGraph Platform's infrastructure, when troubleshooting deployment issues, or when planning a LangGraph implementation. (Technical overview of LangGraph Platform architecture, explaining how Postgres is used for persistence of user data, runs, and long-term memory, while Redis is used for worker communication and ephemeral metadata storage. Includes details on communication mechanisms and data handling between components.)

[LangGraph's Runtime (Pregel)](https://langchain-ai.github.io/langgraph/concepts/pregel/): LLM should read this page when needing to understand LangGraph's runtime architecture, implementing direct Pregel applications, or troubleshooting graph execution behavior. (LangGraph's Pregel runtime manages execution through actors and channels in a Bulk Synchronous Parallel model with three phases: Plan, Execution, and Update. It provides both basic channels (LastValue, Topic) and advanced channels (Context, BinaryOperatorAggregate) for communication between nodes, with examples of direct implementation and comparison to high-level APIs like StateGraph and Functional API.)

[LangGraph Platform: Scalability & Resilience](https://langchain-ai.github.io/langgraph/concepts/scalability_and_resilience/): LLM should read this page when needing to understand LangGraph Platform's scalability capabilities, when evaluating deployment architecture for high-availability applications, or when troubleshooting resilience mechanisms. (Explains how LangGraph Platform achieves horizontal scalability through stateless server design, queue scalability with concurrent runs, and system resilience through heartbeat mechanisms, Postgres database backups, and Redis for ephemeral metadata storage. Details how the platform handles failure scenarios and maintains high availability.)

[LangGraph SDK](https://langchain-ai.github.io/langgraph/concepts/sdk/): LLM should read this page when seeking information about the LangGraph SDK, when needing to interact with LangGraph Server API, or when deciding between Python and JavaScript implementations. (The LangGraph SDK provides client libraries for both Python and JavaScript to interact with the LangGraph Server API. The page covers installation instructions, API reference links, and examples of both synchronous and asynchronous client usage in Python.)

[Self-Hosted](https://langchain-ai.github.io/langgraph/concepts/self_hosted/): LLM should read this page when looking for self-hosted LangGraph deployment options, understanding LangGraph's enterprise vs lite versions, or researching requirements for self-hosted deployment. (The page explains the two versions of self-hosted LangGraph deployments: Self-Hosted Lite (limited to 1M nodes/year) and Self-Hosted Enterprise (requires license key). It covers deployment requirements, how self-hosting works with Redis and Postgres, and provides information about available Helm charts for Kubernetes deployment.)

[Streaming](https://langchain-ai.github.io/langgraph/concepts/streaming/): LLM should read this page when needing to implement streaming capabilities in LangGraph applications, understanding different streaming modes, or building responsive LLM applications. (This page explains streaming in LangGraph, covering methods like .stream and .astream with their various modes (values, updates, custom, messages, debug), how to combine multiple streaming modes, and how streaming works in the LangGraph Platform for creating responsive applications with real-time updates.)

[Template Applications](https://langchain-ai.github.io/langgraph/concepts/template_applications/): LLM should read this page when looking for LangGraph template applications, needing to understand available starter templates, or learning how to create a new LangGraph app with the CLI. (The page covers LangGraph template applications, including how to install the LangGraph CLI, available template options for Python and JS/TS (basic project, ReAct Agent, Memory Agent, Retrieval Agent, and Data-Enrichment Agent), steps to create a new app using templates, and next steps for deployment and further learning.)

[Time Travel ‚è±Ô∏è](https://langchain-ai.github.io/langgraph/concepts/time-travel/): LLM should read this page when needing to understand time travel debugging in LangGraph, when troubleshooting agent decision paths, or when exploring alternative execution paths. (Time Travel in LangGraph includes two key techniques: Replaying, which allows revisiting past agent actions up to a specific checkpoint, and Forking, which enables exploring alternative execution paths from a previous checkpoint.)

[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/v0-human-in-the-loop/): LLM should read this page when needing to implement human-in-the-loop patterns with LangGraph, understanding interaction patterns for agent workflows, or learning about interrupting agent execution for human input. (This page explains human-in-the-loop capabilities in LangGraph, covering persistence, breakpoints, interaction patterns (approval, editing, input), and use cases like reviewing tool calls and time travel (replaying/forking).)

[How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/): LLM should read this page when (looking for specific how-to guides for LangGraph, needing to implement specific LangGraph functionality, or troubleshooting LangGraph deployment issues) (Comprehensive index of how-to guides for LangGraph covering Graph API basics, state management, persistence, memory, human-in-the-loop interactions, streaming, tool calling, multi-agent systems, and LangGraph Platform deployment options and features)

[How to implement handoffs between agents](https://langchain-ai.github.io/langgraph/how-tos/agent-handoffs/): LLM should read this page when (implementing agent handoffs in multi-agent systems) (creating tool-calling agents that can transfer control to other agents) (developing complex multi-agent networks) (The page explains two approaches to implement handoffs between agents in LangGraph: using Command objects for direct control flow and state updates, or using handoff tools that return Command objects for tool-calling agents. Both methods allow agents to pass control to other agents with optional payloads and state updates.)

[How to use LangGraph Platform to deploy CrewAI, AutoGen, and other frameworks](https://langchain-ai.github.io/langgraph/how-tos/autogen-langgraph-platform/): LLM should read this page when needing to deploy third-party agent frameworks to LangGraph Platform, when integrating AutoGen or CrewAI with LangGraph, or when seeking deployment options for non-LangGraph agents. (This page explains how to deploy agents from frameworks like AutoGen and CrewAI using LangGraph Platform by wrapping them in a single LangGraph node, including setup steps, defining an AutoGen agent, wrapping it in LangGraph with proper schemas, and deployment instructions.)

[How to combine control flow and state updates with Command](https://langchain-ai.github.io/langgraph/how-tos/command/): LLM should read this page when needing to combine state updates with control flow in LangGraph, or when navigating between nodes in parent-child graph structures. (This page explains how to use the Command object in LangGraph to combine state updates with control flow, allowing nodes to both update state and determine the next node to execute, including how to navigate from subgraphs to parent graphs using Command.PARENT.)

[How to add runtime configuration to your graph](https://langchain-ai.github.io/langgraph/how-tos/configuration/): LLM should read this page when needing to add configuration options to LangGraph, when customizing graph behavior at runtime, or when implementing model selection in a graph. (This page demonstrates how to add runtime configuration to LangGraph workflows, including how to pass configuration parameters like model selection and system messages through the configurable dictionary in RunnableConfig, with complete code examples showing implementation patterns.)

[How to use the pre-built ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/): LLM should read this page when (needing to implement a ReAct agent, setting up a simple tool-enabled agent, learning basic LangGraph usage) (Tutorial on using the pre-built ReAct agent in LangGraph, showing how to create an agent with a weather tool, initialize the model and tools, define the graph, and run the agent with both tool-requiring and non-tool-requiring queries)

[How to add human-in-the-loop processes to the prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-hitl/): LLM should read this page when (implementing human-in-the-loop capabilities with ReAct agents, troubleshooting tool calls in ReAct agents, building interactive AI systems that require human intervention) (Tutorial on adding human-in-the-loop processes to prebuilt ReAct agents, using interrupt_before=["tools"] parameter to create breakpoints before tool execution, implementing manual approval or editing of tool calls, and resuming execution with updated state)

[How to manage conversation history in a ReAct Agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-manage-message-history/): LLM should read this page when needing to implement message history management in a ReAct agent or when resolving context window limitations in an agent. (Guide explaining three approaches to manage conversation history in ReAct agents: message trimming with original history preservation, overwriting original message history, and message summarization, with code examples for each implementation method)

[How to add thread-level memory to a ReAct Agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/): LLM should read this page when (needing to add persistent memory to a ReAct Agent, implementing thread-level chat history in agents, or building conversational agents that maintain context) (Tutorial on adding thread-level memory to a ReAct Agent using MemorySaver checkpointer with LangGraph, including code examples showing how to initialize the memory component, create the agent with the checkpointer, and preserve conversation context between interactions using thread IDs)

[How to return structured output from the prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-structured-output/): LLM should read this page when needing to configure structured output from a ReAct agent, when customizing agent response formats, or when implementing schema validation in agent responses. (Guide demonstrating how to create a ReAct agent that returns structured output by providing a response_format parameter with a Pydantic schema, including how to customize the prompt for structured output generation and how to access the structured response in the agent output)

[How to add a custom system prompt to the prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-system-prompt/): LLM should read this page when (needing to customize system prompts for ReAct agents, implementing language-specific responses in agents, or modifying agent behavior) (Tutorial showing how to add custom system prompts to prebuilt ReAct agents in LangGraph by passing a string to the prompt parameter, with examples demonstrating implementation and usage resulting in language-specific responses)

[How to add cross-thread persistence (functional API)](https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence-functional/): LLM should read this page when implementing cross-thread persistence in LangGraph with functional API, or when using shared memory across different conversation threads. (Tutorial on implementing cross-thread persistence with LangGraph's functional API using Store interface, including setup with InMemoryStore, creating embeddings for memory search, and building a workflow that stores and retrieves user information across different conversation threads)

[How to disable streaming for models that don't support it](https://langchain-ai.github.io/langgraph/how-tos/disable-streaming/): LLM should read this page when needing to handle models that don't support streaming or when encountering errors with the astream_events API. (This page demonstrates how to disable streaming for models that don't support it by setting the disable_streaming=True parameter on a chat model, allowing it to work properly with the astream_events API without errors.)

[How to add dynamic breakpoints with NodeInterrupt](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/dynamic_breakpoints/): LLM should read this page when needing to implement dynamic breakpoints in LangGraph workflows, when adding conditional human intervention points, or when troubleshooting graph interruptions. (Guide on implementing NodeInterrupt in LangGraph for dynamic breakpoints, including how to define interrupt conditions within nodes, how to handle interrupted states, and how to update graph state to resume execution after an interruption.)

[How to Review Tool Calls](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/review-tool-calls/): LLM should read this page when (needing to implement human review of LLM tool calls, designing human-in-the-loop systems, creating approval workflows for agent actions) (Tutorial on implementing tool call review functionality in LangGraph, covering approval workflows, tool call editing, and providing feedback. Includes code examples for creating interrupts that allow humans to review, modify or provide feedback on tool calls before execution.)

[How to view and update past graph state](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/): LLM should read this page when (learning how to access or modify previous states in LangGraph workflows) (implementing time travel functionality for agent debugging) (creating user controls for agent decision paths) (Tutorial showing how to view, replay, and branch off past graph states using get_state and update_state methods with checkpointing, allowing time travel and state manipulation in LangGraph agents)

[How to wait for user input using interrupt](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/): LLM should read this page when (wanting to implement user input capabilities in LangGraph flows) (learning how to pause and resume graph execution to wait for human feedback) (This page demonstrates how to use the interrupt() function in LangGraph to pause graph execution, collect user input, and then continue execution with that input. It includes examples of simple usage patterns, implementation in an agent context, and shows how to use Command(resume=...) to provide user responses.)

[How to define input/output schema for your graph](https://langchain-ai.github.io/langgraph/how-tos/input_output_schema/): LLM should read this page when needing to implement different input/output schemas for LangGraph, when filtering graph outputs, or when working with mixed state schemas. (This page explains how to define distinct input and output schemas for a StateGraph in LangGraph, where input schemas validate incoming data and output schemas filter what gets returned, with a complete code example demonstrating implementation.)

[How to connect a local agent to LangGraph Studio](https://langchain-ai.github.io/langgraph/how-tos/local-studio/): LLM should read this page when setting up local agent visualization, connecting agents to Studio UI, or debugging LangGraph agents locally. (This page explains how to connect a local LangGraph agent to LangGraph Studio for visualization and debugging, including: setting up langgraph.json, installing langgraph-cli with inmem extras, running the development server with "langgraph dev", using the cloud-hosted Studio UI with locally running agents, and attaching optional debuggers.)

[How to create map-reduce branches for parallel execution](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/): LLM should read this page when needing to create parallel processing workflows with map-reduce patterns or when implementing fan-out/fan-in operations for LLM tasks. (This page explains how to create map-reduce branching in LangGraph for parallel execution, using the Send API to distribute different states to multiple instances of a node and then combine results, with a complete example of generating jokes on multiple subjects and selecting the best one.)

[How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/): LLM should read this page when implementing conversation summarization features, managing long context windows, or building chatbots with memory optimization. (This page demonstrates how to create a chatbot with conversation summarization capabilities using LangGraph, showing how to detect when a conversation grows too long, generate summaries, remove old messages while preserving context, and maintain conversational continuity with a smaller context window.)

[How to delete messages](https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages/): LLM should read this page when looking to delete messages from an agent's conversation history, when managing memory in LangGraph applications, or when implementing message pruning strategies. (This page explains how to delete messages from a graph's message state, both manually and programmatically. It covers using the RemoveMessage modifier, working with MessagesState's reducer, and demonstrates how to delete specific messages by ID or implement automatic deletion of older messages.)

[How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/): LLM should read this page when needing to implement conversation history management, prevent context window overflows, or filter messages in LangGraph agents. (This page explains how to manage conversation history in LangGraph agents, including setting up a basic ReAct agent, implementing message filtering to prevent context window overflow, and references to additional LangChain utilities for message management and trimming.)

[How to add semantic search to your agent's memory](https://langchain-ai.github.io/langgraph/how-tos/memory/semantic-search/): LLM should read this page when seeking to implement semantic search within agent memory, needing to enhance agent recall, or customizing memory indexing strategies. (Guide on implementing semantic search with LangGraph memories, including basic setup, integration with agents, multi-vector indexing, field-specific embedding, and selective memory indexing)

[How to add multi-turn conversation in a multi-agent application](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo/): LLM should read this page when implementing multi-turn conversations in multi-agent systems or needing to add user interaction between agent handoffs. (This guide demonstrates how to create a multi-agent application that enables multi-turn conversations between users and agents using LangGraph. It covers creating a human node with interrupts to collect user input, routing between agents using Commands, and implementing handoffs between specialized agents while maintaining conversation continuity.)

[How to add multi-turn conversation in a multi-agent application (functional API)](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-multi-turn-convo-functional/): LLM should read this page when (implementing multi-turn conversations in multi-agent systems, working with agent handoffs using the functional API, or building interactive workflows requiring user input) (Tutorial on creating a multi-agent application with multi-turn conversation capabilities using LangGraph's functional API, including setup for agents that can converse with users and hand off to each other, implementation of interrupts for user input, and complete code examples with travel and hotel advisor agents)

[How to build a multi-agent network](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network/): LLM should read this page when (building multi-agent networks, implementing agent handoffs, creating connected agent systems) (Demonstrates how to build a multi-agent network architecture where agents can communicate with each other through handoffs. Shows two implementation approaches: using custom agent implementation with travel and hotel advisors, and using prebuilt ReAct agents with tools. Includes code examples for defining agent nodes and connecting them in a StateGraph.)

[How to build a multi-agent network (functional API)](https://langchain-ai.github.io/langgraph/how-tos/multi-agent-network-functional/): LLM should read this page when (implementing multi-agent networks using LangGraph's functional API, creating agent handoffs between specialized AI agents, building fully-connected agent communication networks) (Tutorial demonstrating how to create a multi-agent network where agents can communicate with each other and hand off tasks using LangGraph's functional API, including the setup of travel advisor and hotel advisor agents with specialized tools, proper task management, and implementing agent transfers through the entrypoint function)

[How to add node retry policies](https://langchain-ai.github.io/langgraph/how-tos/node-retries/): LLM should read this page when needing to implement retry logic in LangGraph nodes, handling error-prone operations, or building fault-tolerant LLM applications. (This page explains how to add retry policies to nodes in a LangGraph, including configuring retry parameters like max attempts, backoff factors, and which exceptions to retry on. It demonstrates setting up RetryPolicy objects and applying them to specific nodes in a graph.)

[How to pass private state between nodes](https://langchain-ai.github.io/langgraph/how-tos/pass_private_state/): LLM should read this page when needing to pass data selectively between graph nodes, when implementing private state in LangGraph, or when designing complex node communication patterns. (This page demonstrates how to share private information between specific nodes in a LangGraph without including it in the main graph state, using TypedDict classes to define different schemas for node inputs/outputs and controlling what data is accessible to each node in the execution flow.)

[How to add thread-level persistence to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/): LLM should read this page when needing to implement state persistence in LangGraph, understanding thread-level memory for chatbots, or learning how to manage conversation history across interactions. (This page explains how to add thread-level persistence to LangGraph applications by using a checkpointer when compiling a graph, allowing conversations to maintain context across multiple interactions with the same thread_id, with code examples using MemorySaver.)

[How to use MongoDB checkpointer for persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence_mongodb/): LLM should read this page when (needing to implement MongoDB for state persistence in LangGraph, wanting to add memory to agents using MongoDB, or exploring different connection methods for MongoDB checkpointing) (This page demonstrates how to use MongoDB as a backend for persisting LangGraph state, covering four different connection methods: using a connection string, using the MongoDB client, using an async connection, and using the async MongoDB client)

[How to use Postgres checkpointer for persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/): LLM should read this page when implementing persistence with Postgres in LangGraph, when setting up checkpointing for agents, or when needing database connection examples. (Guide on using PostgreSQL for persisting LangGraph agent state, with examples for both synchronous and asynchronous connections using connection pools, direct connections, and connection strings.)

[How to create a custom checkpointer using Redis](https://langchain-ai.github.io/langgraph/how-tos/persistence_redis/): LLM should read this page when (needing to implement persistent state in LangGraph using Redis, creating a custom checkpointer for agent memory, integrating Redis with LangGraph applications) (Complete guide on creating custom Redis-based checkpointing for LangGraph, including sync and async implementations, serialization methods, and examples of using the checkpointer with a ReAct agent)

[How to create a ReAct agent from scratch](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/): LLM should read this page when implementing custom ReAct agents, when modifying agent state structure, or when creating tool-calling workflows from scratch. (This page demonstrates how to build a ReAct agent from scratch using LangGraph, covering graph state definition, model and tool setup, node and edge configuration, graph compilation, and practical usage examples for implementing custom reasoning workflows).

[How to create a ReAct agent from scratch (Functional API)](https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch-functional): LLM should read this page when (trying to create a ReAct agent from scratch using the Functional API, implementing tool-calling capabilities in a conversational agent, or learning about thread-level persistence in LangGraph) (Tutorial for creating a ReAct agent using LangGraph's Functional API, covering model and tool definition, task implementation, entrypoint creation, and adding thread-level persistence for conversational capabilities)

[How to return state before hitting recursion limit](https://langchain-ai.github.io/langgraph/how-tos/return-when-recursion-limit-hits/): LLM should read this page when needing to handle recursion limits in LangGraph, preventing graph execution errors, or returning final state data before hitting limits. (A guide showing how to modify a StateGraph to gracefully terminate execution before hitting recursion limits by using the RemainingSteps annotation, with examples of both problematic code that throws errors and the solution with proper state management.)

[How to review tool calls (Functional API)](https://langchain-ai.github.io/langgraph/how-tos/review-tool-calls-functional/): LLM should read this page when (needing to implement human review of tool calls in LangGraph, implementing interrupts for tool validation, or building a Functional API ReAct agent with human oversight) (This page demonstrates how to implement human-in-the-loop workflows to review tool calls before execution in a ReAct agent using LangGraph's Functional API, including how to accept, revise, or provide custom feedback on tool calls through an interrupt mechanism)

[How to pass custom run ID or set tags and metadata for graph runs in LangSmith](https://langchain-ai.github.io/langgraph/how-tos/run-id-langsmith/): LLM should read this page when needing to pass custom run IDs in LangSmith, setting tags or metadata for graph runs, or tracking execution details in LangGraph. (This page explains how to add custom metadata to LangGraph runs in LangSmith, covering how to pass run_id, tags, and metadata to track execution, with code examples showing how to use the config parameter to provide this information when invoking graphs.)

[How to use Pydantic model as graph state](https://langchain-ai.github.io/langgraph/how-tos/state-model/): LLM should read this page when (needing to validate graph state data, implementing Pydantic models in LangGraph, troubleshooting type validation errors) (Tutorial on using Pydantic models as graph state in LangGraph, covering input validation, multi-node graphs, serialization behavior, runtime type coercion, and working with message models, with limitations noted including validation only on inputs)

[How to stream](https://langchain-ai.github.io/langgraph/how-tos/streaming/): LLM should read this page when needing to implement streaming in LangGraph, when troubleshooting streaming issues, or when looking to improve application responsiveness. (This page explains how to stream data from LangGraph using various streaming modes: "values" (all state values), "updates" (node updates only), "debug" (detailed step information), "messages" (LLM tokens), and "custom" (user-defined data). It includes code examples for each streaming mode and how to combine multiple modes.)

[How to stream data from within a tool](https://langchain-ai.github.io/langgraph/how-tos/streaming-events-from-within-tools/): LLM should read this page when trying to implement streaming from tools, handling tool output incrementally, or streaming LLM tokens from within tools. (This page explains how to stream data from within a tool in LangGraph, covering streaming custom data, streaming LLM tokens, and implementing streaming without LangChain, with code examples for each approach.)

[How to stream from subgraphs](https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/): LLM should read this page when needing to implement streaming from subgraphs in LangGraph or when troubleshooting streaming issues with nested graph structures. (This page explains how to stream outputs from subgraphs in LangGraph by using the subgraphs=True parameter in the parent graph's .stream() method, including a complete example demonstrating how to access both parent graph and subgraph node updates with their respective namespaces.)

[How to stream LLM tokens from your graph](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens): LLM should read this page when needing to implement token streaming from LLMs in LangGraph, when filtering specific LLM outputs in a graph, or when implementing streaming without LangChain. (Detailed guide on how to stream individual LLM tokens from LangGraph nodes using graph.stream() with stream_mode="messages", including examples for filtering specific LLM invocations using metadata and tags, and implementing custom streaming without LangChain using stream_mode="custom".)

[How to use subgraphs](https://langchain-ai.github.io/langgraph/how-tos/subgraph/): LLM should read this page when learning to use subgraphs in LangGraph, when needing to compose complex workflows, or when building nested state management systems. (This guide explains how to incorporate subgraphs in LangGraph applications through two methods: adding a compiled subgraph as a node when parent and subgraph share state keys, or creating a node function that transforms state between parent and subgraph when they have different schemas.)

[How to add thread-level persistence to a subgraph](https://langchain-ai.github.io/langgraph/how-tos/subgraph-persistence): LLM should read this page when implementing persistence in LangGraph applications with subgraphs, or when needing to debug state persistence across nested graphs. (This page explains how to add thread-level persistence to subgraphs in LangGraph by passing a checkpointer only to the parent graph during compilation, which automatically propagates to child subgraphs. It includes code examples showing setup, implementation, and verification of persistence for both parent and child graphs.)

[How to view and update state in subgraphs](https://langchain-ai.github.io/langgraph/how-tos/subgraphs-manage-state/): LLM should read this page when learning how to access and modify state within subgraphs, handling breakpoints in nested graph flows, or implementing human-in-the-loop patterns. (Guide shows how to view and update state in subgraphs using persistence, including resuming execution from breakpoints, modifying state before execution continues, acting as a specific node in a subgraph, and handling these operations in multilevel nested subgraphs.)

[How to interact with the deployment using RemoteGraph](https://langchain-ai.github.io/langgraph/how-tos/use-remote-graph/): LLM should read this page when (needing to interact with deployed LangGraph applications, working with RemoteGraph, or integrating remote graphs as subgraphs) (This page explains how to use RemoteGraph to interact with LangGraph Platform deployments, covering initialization methods (URL or client-based), invoking graphs synchronously and asynchronously, thread-level persistence for stateful interactions, and using RemoteGraph as a subgraph in larger applications)

[How to wait for user input (Functional API)](https://langchain-ai.github.io/langgraph/how-tos/wait-user-input-functional/): LLM should read this page when (implementing human-in-the-loop workflows in LangGraph, adding user input pauses to agents, or using the Functional API for interactive applications) (Guide showing how to implement "wait for user input" functionality in LangGraph using the Functional API with interrupt() function, including a simple example and integration with a ReAct agent that can pause execution to request human assistance before continuing)

[Learn the basics](https://langchain-ai.github.io/langgraph/tutorials/introduction/): LLM should read this page when (learning LangGraph basics, trying to build chatbots with tools, exploring human-in-the-loop AI implementations) (Step-by-step tutorial for building a support chatbot with LangGraph, covering basic chatbot creation, adding search tools, implementing memory with checkpointing, human-in-the-loop interactions, custom state management, and time travel capabilities for exploring alternative conversation paths)

[Local Deploy](https://langchain-ai.github.io/langgraph/tutorials/langgraph-platform/local-server/): LLM should read this page when wanting to set up a local LangGraph server, when looking for instructions to create and test a LangGraph app locally, or when needing to understand local deployment options for LangGraph. (Step-by-step guide for local LangGraph deployment, including installing CLI, creating an app from templates, setting up dependencies, configuring environment variables, launching the server, using the Studio Web UI, testing the API with various SDKs, and next steps for cloud deployment)

[Workflows and Agents](https://langchain-ai.github.io/langgraph/tutorials/workflows/): LLM should read this page when exploring implementation patterns for AI workflows, comparing agent vs workflow architectures, or building structured LLM systems with LangGraph. (Comprehensive guide to common workflow and agent patterns including prompt chaining, parallelization, routing, orchestrator-worker, evaluator-optimizer, and agent architectures, with code examples in both Graph API and Functional API styles, plus explanation of LangGraph features like persistence, human-in-the-loop, memory, streaming, and deployment options.)

[Introduction | ü¶úÔ∏èüîó LangChain](https://python.langchain.com/docs/get_started/introduction/): LLM should read this page when seeking an introduction to LangChain framework or understanding its core architecture and components. (Introduction to the LangChain framework for developing LLM-powered applications, including its architecture components (langchain-core, integration packages, langchain, langchain-community, langgraph), guides to tutorials/how-tos/concepts, ecosystem tools like LangSmith and LangGraph, and links to documentation resources.)

[Page Not Found | ü¶úÔ∏èüîó LangChain](https://python.langchain.com/docs/modules/): LLM should read this page when (trying to resolve broken links on LangChain website or understanding LangChain's error pages) (This is a 404 "Page Not Found" error page from the LangChain website, containing navigation links to other LangChain resources including Integrations, API Reference, LangSmith, LangGraph, and community links but no actual content for the requested page)

[Page Not Found | ü¶úÔ∏èüîó LangChain](https://python.langchain.com/docs/use_cases/): LLM should read this page when looking for broken links on the LangChain website or trying to navigate to a non-existent LangChain page. (This is a 404 error page from LangChain's website, informing users that the requested page could not be found and suggesting they contact the site owner who linked them to the broken URL.)
