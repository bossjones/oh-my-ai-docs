---
description: "This rule governs the essential MVP testing patterns for FastMCP servers. It should be applied whenever: (1) Creating new tests for FastMCP components, (2) Implementing basic TDD patterns, (3) Setting up minimal test environments, or (4) Verifying core functionality. The rule ensures consistent testing patterns and reliable verification of basic functionality across FastMCP server implementations."
globs: tests/**/*.py
alwaysApply: false
---

# FastMCP MVP Testing Patterns

@python-tdd-auto.mdc should be used with this rule as well.

## Critical Rules

### Pre-Implementation Investigation
- ALWAYS run `tree tests` to verify test folder structure before writing any test code
- ALWAYS search for fixture references in conftest.py and related test files
- ALWAYS use grep/search to find similar tests or patterns before writing new ones
- ALWAYS use `<quotes>...</quotes>` tags when quoting relevant code from existing files
- ALWAYS use `<thinking>...</thinking>` to document your reasoning process before writing any code
- NEVER generate test code without first examining folder structure and fixture references

### Code Reference Discovery
- ALWAYS use `grep_search` or `codebase_search` to find related patterns before implementation
- ALWAYS check for related fixtures with `grep_search -q "def \w+_fixture" tests/`
- ALWAYS search for similar test implementations with `grep_search -q "def test_\w+" tests/`
- ALWAYS examine conftest.py files in relevant test directories
- ALWAYS review imports and dependencies in similar test files
- NEVER assume fixture availability without explicit verification
- NEVER implement tests without checking existing patterns first

### Reference Documentation
- ALWAYS consult SCRATCH.md when unsure about testing patterns or implementation details
- Key sections in SCRATCH.md for reference:
  * `## Infrastructure Setup` - For server and transport configuration
  * `## Logging Implementation` - For logging patterns and structured data
  * `## Testing Components` - For detailed testing patterns by component
  * `## Error Handling` - For error tracking and recovery patterns
  * `## Security Measures` - For security-related testing
  * `## Performance Monitoring` - For performance testing patterns
  * `## Documentation` - For documentation standards
- Use these section headers to search for specific implementation details and examples
- When searching, use exact section headers for precise results

### Test Infrastructure
- ALWAYS use `create_connected_server_and_client_session` alias as `client_session`
- ALWAYS ensure proper server instance handling via `mcp._mcp_server`
- ALWAYS use in-memory connections for test isolation
- NEVER use real network connections in tests
- NEVER share server instances between tests

### Test Structure and Organization
- ALWAYS organize tests by component functionality
- ALWAYS use descriptive test names that indicate what is being tested
- ALWAYS include docstrings in test classes and methods
- ALWAYS respect existing folder structure and organization patterns
- ALWAYS follow fixture naming conventions from existing code
- NEVER mix different component tests in the same class
- NEVER create new test patterns without checking existing ones

### Async Testing
- ALWAYS use `@pytest.mark.anyio` for async tests
- ALWAYS use async context managers for resource management
- ALWAYS await async operations properly
- NEVER mix sync and async code in the same test
- NEVER use bare `asyncio` calls when `pytest.mark.anyio` is available

### Resource Management
- ALWAYS use context managers for resource cleanup
- ALWAYS use temporary resources for file/network operations
- ALWAYS clean up resources in test teardown
- ALWAYS test resource registration using `@mcp.resource`
- ALWAYS verify resource content and iteration patterns
- ALWAYS test resource cleanup in async contexts
- NEVER leave resources uncleaned between tests
- NEVER use global resources without proper isolation

### Mocking Philosophy
- ALWAYS prefer real implementations over mocks when practical
- ALWAYS limit mocking to external side effects (logging, file I/O, progress reports)
- **NEVER mock `Context.info`, `Context.debug`, or `Context.report_progress` directly.** Use `mocker.patch("mcp.server.session.ServerSession.send_log_message")` or `mocker.patch("mcp.server.session.ServerSession.send_progress_update")` to verify log messages and progress updates instead.
- ALWAYS use proper mock assertions when mocks are needed
- NEVER mock core FastMCP functionality
- NEVER use mocks just to simplify tests

### Type Safety
- ALWAYS include proper type hints in test functions
- ALWAYS test with typed parameters and return values
- ALWAYS verify type compatibility in interface tests
- NEVER use `Any` type in test code
- NEVER ignore type checking in tests

### Test Coverage
- ALWAYS include happy path and error path tests
- ALWAYS test boundary conditions
- ALWAYS include integration tests for component interaction
- NEVER test only the happy path
- NEVER skip error handling tests

### Client-Server Testing
- ALWAYS use `client_session` for client-server tests
- ALWAYS test connection lifecycle (connect, operate, disconnect)
- ALWAYS verify request-response patterns
- ALWAYS test client tool invocation patterns
- ALWAYS test client response handling
- ALWAYS test client error scenarios
- ALWAYS test client connection states
- NEVER leave connections open between tests
- NEVER skip testing error conditions in client-server communication

### Context Testing
- ALWAYS test context detection and injection explicitly
- ALWAYS verify all required context attributes (e.g., request_id)
- ALWAYS test all logging levels systematically (debug, info, warning, error)
- ALWAYS test both required and optional context scenarios
- ALWAYS verify context resource access patterns
- ALWAYS test context resource iteration (`r_iter = await ctx.read_resource()`)
- ALWAYS test optional context parameters
- NEVER skip testing context attribute validation
- NEVER assume context attributes without verification

### Tool Testing
- ALWAYS test tool registration using `mcp._tool_manager.add_tool`
- ALWAYS test tool parameter detection
- ALWAYS test tool return value handling
- ALWAYS test tool async/sync compatibility
- NEVER skip tool registration verification
- NEVER assume tool parameters without testing

### Integration Testing
- ALWAYS test tool-resource interactions
- ALWAYS test context-resource interactions
- ALWAYS test client-server communication patterns
- ALWAYS test end-to-end workflows
- NEVER test components in isolation only
- NEVER skip integration scenarios

### Documentation
- ALWAYS include proper test docstring formats
- ALWAYS follow proper test class organization
- ALWAYS use proper test method naming
- ALWAYS use proper test assertion patterns
- NEVER skip documentation in test code
- NEVER use unclear test names or organization

### Client Session Testing
- ALWAYS verify connection state at each lifecycle stage
- ALWAYS test session cleanup and resource release
- ALWAYS verify client session isolation between tests
- ALWAYS test session reconnection scenarios
- ALWAYS verify session state after errors
- NEVER leave sessions in inconsistent states
- NEVER skip session lifecycle verification

### Verification Layers
- ALWAYS implement all verification layers:
  * Context detection and injection
  * Context attributes and properties
  * Functionality (logging, resources)
  * Async/sync compatibility
  * Error scenarios and recovery
- ALWAYS verify each layer independently
- ALWAYS test layer interactions
- NEVER skip verification layers
- NEVER combine layer verifications without proper isolation

### Test Failure Debugging
- ALWAYS use precise test selection when debugging failures
- ALWAYS generate isolated test commands for each failing test
- ALWAYS use the following command format for debugging individual failing tests:
  ```
  uv run pytest -s --verbose --showlocals --tb=short <test_path>::<TestClass>::<test_method>
  ```
- ALWAYS analyze test failures one by one, starting with the first failure
- ALWAYS use `--showlocals` to inspect the state when failures occur
- ALWAYS use `--tb=short` for cleaner traceback output
- ALWAYS transform pytest failure reports into individual debugging commands, for example:
  ```
  # Transform failure report:
  - tests/test_avectorstore/test_aserver.py:345 TestAVectorStoreMCPServer.test_resource_read_error

  # Into debugging command:
  uv run pytest -s --verbose --showlocals --tb=short tests/test_avectorstore/test_aserver.py::TestAVectorStoreMCPServer::test_resource_read_error
  ```
- NEVER run the entire test suite when debugging specific failures
- NEVER debug multiple test failures simultaneously
- NEVER skip the detailed inspection of test failure output
- ALWAYS use filter flags when needed (e.g., `-k` for keyword filtering)
- ALWAYS consider fixture issues when tests fail unexpectedly
- ALWAYS check for shared state problems between tests when multiple failures occur
- ALWAYS validate mock configuration when mock-related assertions fail

## Examples

<example>
```python
# Good practices demonstrated
import pytest
from typing import AsyncIterator
from mcp.server.fastmcp import FastMCP, Context
from mcp.shared.memory import create_connected_server_and_client_session as client_session
from unittest.mock import patch

class TestServerCore:
    """Core functionality tests for FastMCP server.

    Tests basic server operations, tool management, and error handling.
    """

    @pytest.mark.anyio
    async def test_server_lifecycle(self):
        """Test complete server lifecycle including startup and shutdown."""
        mcp = FastMCP(instructions="Test server")

        async with client_session(mcp._mcp_server) as client:
            # Test server is operational
            assert client.is_connected()

            # Test basic functionality
            @mcp.tool()
            async def echo(msg: str) -> str:
                return msg

            result = await client.call_tool("echo", {"msg": "test"})
            assert result.content[0].text == "test"

        # Verify cleanup after context exit
        assert not mcp._mcp_server.is_running()

    @pytest.mark.anyio
    async def test_resource_management(self):
        """Test resource lifecycle and cleanup."""
        mcp = FastMCP()

        # Define resource with proper typing
        @mcp.resource("test://data")
        async def test_resource() -> AsyncIterator[str]:
            yield "test data"
            # Cleanup happens automatically

        async with client_session(mcp._mcp_server) as client:
            result = await client.read_resource("test://data")
            assert result.contents[0].text == "test data"

    @pytest.mark.anyio
    async def test_error_handling_comprehensive(self):
        """Test various error scenarios and proper error propagation."""
        mcp = FastMCP()

        @mcp.tool()
        async def error_tool(error_type: str) -> None:
            if error_type == "value":
                raise ValueError("Invalid value")
            elif error_type == "key":
                raise KeyError("Missing key")
            raise RuntimeError("Unknown error")

        async with client_session(mcp._mcp_server) as client:
            # Test multiple error types
            for error_type, expected_error in [
                ("value", "Invalid value"),
                ("key", "Missing key"),
                ("unknown", "Unknown error")
            ]:
                result = await client.call_tool(
                    "error_tool",
                    {"error_type": error_type}
                )
                assert result.isError
                assert expected_error in result.content[0].text

    @pytest.mark.anyio
    async def test_context_with_logging(self):
        """Test context functionality with logging verification."""
        mcp = FastMCP()

        @mcp.tool()
        async def logging_tool(msg: str, ctx: Context) -> str:
            await ctx.info(f"Processing: {msg}")
            await ctx.debug("Debug info")
            return f"Processed {msg}"

        # CORRECT: Mock the session's send_log_message method
        with patch("mcp.server.session.ServerSession.send_log_message") as mock_log:
            async with client_session(mcp._mcp_server) as client:
                result = await client.call_tool(
                    "logging_tool",
                    {"msg": "test"}
                )

                # Verify tool result
                assert result.content[0].text == "Processed test"

                # Verify logging side effects via the mocked session method
                mock_log.assert_any_call(
                    level="info",
                    data="Processing: test",
                    logger=None
                )
                mock_log.assert_any_call(
                    level="debug",
                    data="Debug info",
                    logger=None
                )

class TestContextFeatures:
    """Comprehensive context feature testing."""

    @pytest.mark.anyio
    async def test_context_detection(self):
        """Test proper context detection and injection."""
        mcp = FastMCP()

        @mcp.tool()
        async def tool_with_context(x: int, ctx: Context) -> str:
            assert ctx.request_id is not None
            return f"Request {ctx.request_id}: {x}"

        async with client_session(mcp._mcp_server) as client:
            result = await client.call_tool(
                "tool_with_context",
                {"x": 42}
            )
            assert "Request" in result.content[0].text
            assert "42" in result.content[0].text

    @pytest.mark.anyio
    async def test_context_logging_levels(self):
        """Test all context logging levels."""
        mcp = FastMCP()

        @mcp.tool()
        async def logging_tool(ctx: Context) -> str:
            await ctx.debug("Debug message")
            await ctx.info("Info message")
            await ctx.warning("Warning message")
            await ctx.error("Error message")
            return "Logged all levels"

        with patch("mcp.server.session.ServerSession.send_log_message") as mock_log:
            async with client_session(mcp._mcp_server) as client:
                await client.call_tool("logging_tool", {})

                # Verify all log levels
                mock_log.assert_any_call(level="debug", data="Debug message", logger=None)
                mock_log.assert_any_call(level="info", data="Info message", logger=None)
                mock_log.assert_any_call(level="warning", data="Warning message", logger=None)
                mock_log.assert_any_call(level="error", data="Error message", logger=None)

    @pytest.mark.anyio
    async def test_session_lifecycle(self):
        """Test complete client session lifecycle."""
        mcp = FastMCP()

        async with client_session(mcp._mcp_server) as client:
            # Verify initial connection
            assert client.is_connected()

            # Test basic operation
            @mcp.tool()
            async def ping() -> str:
                return "pong"

            result = await client.call_tool("ping", {})
            assert result.content[0].text == "pong"

            # Test error handling
            @mcp.tool()
            async def error_tool() -> None:
                raise ValueError("Test error")

            error_result = await client.call_tool("error_tool", {})
            assert error_result.isError
            assert "Test error" in error_result.content[0].text

            # Verify connection still valid after error
            assert client.is_connected()

        # Verify cleanup after context exit
        assert not client.is_connected()

    # Example of debugging test failures
    @pytest.mark.anyio
    async def test_failure_debugging_example(self):
        """Example showing how to debug failures."""
        # When failures occur, run tests individually with:
        # uv run pytest -s --verbose --showlocals --tb=short tests/path/test_file.py::TestClass::test_method

        # For example, if the following test fails:
        # tests/test_avectorstore/test_aserver.py:345 TestAVectorStoreMCPServer.test_resource_read_error

        # Run it with:
        # uv run pytest -s --verbose --showlocals --tb=short tests/test_avectorstore/test_aserver.py::TestAVectorStoreMCPServer::test_resource_read_error

        # This allows focused debugging of one failure at a time
        pass
```
</example>

<example type="invalid">
```python
# Bad practices to avoid
class TestBadPractices:
    def test_without_anyio(self):  # ❌ Missing anyio marker
        mcp = FastMCP()
        result = mcp.do_something()  # Not async
        assert result

    @pytest.mark.anyio
    async def test_excessive_mocking(self):  # ❌ Over-mocking
        with patch("mcp.server.FastMCP") as mock_mcp:  # Don't mock core functionality
            mock_mcp.return_value.do_something.return_value = "result"
            result = await mock_mcp.do_something()
            assert result == "result"

    @pytest.mark.anyio
    async def test_poor_resource_management(self):  # ❌ Bad resource management
        mcp = FastMCP()
        client = await mcp._mcp_server.connect()  # No context manager
        result = await client.call_tool("tool", {})
        # Missing cleanup

    @pytest.mark.anyio
    async def test_untyped_code(self):  # ❌ Missing type hints
        mcp = FastMCP()

        @mcp.tool()
        def untyped_tool(x, y):  # Missing type hints
            return x + y

    @pytest.mark.anyio
    async def test_incomplete_error_handling(self):  # ❌ Insufficient error testing
        mcp = FastMCP()
        try:
            result = await mcp.do_something()
        except:  # Too broad exception handling
            pass  # No assertions on error

class TestBadContextPractices:
    @pytest.mark.anyio
    async def test_incomplete_context_testing(self):  # ❌ Missing comprehensive context testing
        mcp = FastMCP()

        @mcp.tool()
        async def bad_tool(ctx: Context) -> str:
            return "No context verification"  # Missing context attribute verification

    @pytest.mark.anyio
    async def test_poor_session_management(self):  # ❌ Poor session handling
        mcp = FastMCP()
        client = await mcp._mcp_server.connect()  # No context manager

        try:
            result = await client.call_tool("some_tool", {})
        finally:
            await client.disconnect()  # Manual cleanup instead of context manager

    @pytest.mark.anyio
    async def test_incomplete_logging_test(self):  # ❌ Incomplete logging verification
        mcp = FastMCP()

        @mcp.tool()
        async def logging_tool(ctx: Context) -> str:
            await ctx.info("Only testing info level")  # Missing other log levels
            return "Done"

        with patch("mcp.server.session.ServerSession.send_log_message") as mock_log:
            async with client_session(mcp._mcp_server) as client:
                await client.call_tool("logging_tool", {})
                # Only verifying one log level
                mock_log.assert_called_once()

# Bad practices - DO NOT DO THIS
import pytest
from mcp.server.fastmcp import FastMCP, Context
from mcp.shared.memory import create_connected_server_and_client_session as client_session
from unittest.mock import patch, AsyncMock

class TestServerCoreInvalid:
    @pytest.mark.anyio
    async def test_incorrect_context_mocking(self, mocker):
        """Demonstrates incorrect mocking of Context methods."""
        mcp = FastMCP()

        @mcp.tool()
        async def logging_tool_bad(msg: str, ctx: Context) -> str:
            await ctx.info(f"Processing: {msg}")
            await ctx.debug("Debug info")
            return f"Processed {msg}"

        # INCORRECT: Mocking Context methods directly
        # This bypasses the actual logging mechanism via the session
        mock_ctx_info = mocker.patch("mcp.server.fastmcp.Context.info", new_callable=AsyncMock)
        mock_ctx_debug = mocker.patch("mcp.server.fastmcp.Context.debug", new_callable=AsyncMock)

        async with client_session(mcp._mcp_server) as client:
            result = await client.call_tool(
                "logging_tool_bad",
                {"msg": "test"}
            )

            # Assertions on these mocks are misleading as they don't reflect
            # what the client would actually receive.
            mock_ctx_info.assert_called_once_with(f"Processing: test")
            mock_ctx_debug.assert_called_once_with("Debug info")

    @pytest.mark.anyio
    async def test_mixing_sync_async(self):
        """Demonstrates mixing sync and async code incorrectly."""
        # This test is invalid because it doesn't use @pytest.mark.anyio
        # and attempts to use async features in a synchronous test function.
        import asyncio
        mcp = FastMCP()

        @mcp.tool()
        async def async_tool() -> str:
            await asyncio.sleep(0.01)
            return "done"

        # Running async code directly in a sync test is problematic
        # loop = asyncio.get_event_loop()
        # loop.run_until_complete(async_tool())
        # This test setup is fundamentally flawed for pytest-anyio
        pass # Placeholder

    # INCORRECT: Bad approach to test failure debugging
    def test_poor_failure_debugging(self):
        """Demonstrates poor approach to debugging test failures."""
        # ❌ Running the entire test suite when you have specific failures
        # pytest tests/

        # ❌ Running multiple failing tests together without focused debugging
        # pytest tests/test_avectorstore/test_aserver.py

        # ❌ Not using proper diagnostic flags
        # pytest tests/test_avectorstore/test_aserver.py::TestAVectorStoreMCPServer::test_resource_read_error

        # ❌ Not examining local variables at point of failure
        # pytest --tb=native tests/test_avectorstore/test_aserver.py::TestAVectorStoreMCPServer::test_resource_read_error
```
</example>
