---
description: This rule governs testing and debugging practices in FastMCP servers. It should be applied whenever: (1) Writing tests for FastMCP components, (2) Implementing debugging strategies, (3) Setting up test environments, (4) Handling error scenarios, or (5) Monitoring and troubleshooting. The rule ensures consistent testing patterns, proper debugging practices, and reliable error handling across all FastMCP server implementations.
globs:
alwaysApply: false
---

# FastMCP Testing and Debugging Rules

## Critical Rules

1. **Unit Testing Patterns**
   - Write comprehensive test cases
   - Use proper test isolation
   - Implement test fixtures
   - Handle async testing correctly
   - Example:
   ```python
   # ✅ Good
   import pytest
   from fastmcp import FastMCP
   from typing import AsyncGenerator
   from unittest.mock import AsyncMock

   @pytest.fixture
   async def test_app() -> AsyncGenerator[FastMCP, None]:
       app = FastMCP()
       await app.startup()
       yield app
       await app.shutdown()

   @pytest.fixture
   def mock_cache() -> AsyncMock:
       return AsyncMock(spec=Cache)

   class TestCacheManager:
       @pytest.mark.asyncio
       async def test_get_or_compute(
           self,
           test_app: FastMCP,
           mock_cache: AsyncMock
       ):
           # Arrange
           key = "test_key"
           expected = {"data": "value"}
           mock_cache.get.return_value = None
           manager = CacheManager(mock_cache)

           # Act
           result = await manager.get_or_compute(
               key,
               lambda: expected,
               timedelta(minutes=5)
           )

           # Assert
           assert result == expected
           mock_cache.set.assert_called_once()

   # ❌ Bad
   def test_bad():
       app = FastMCP()  # No cleanup
       cache = {}  # No proper mocking
       assert cache.get("key") is None  # Too simple
   ```

2. **Integration Testing**
   - Test component interactions
   - Set up test environments
   - Handle external dependencies
   - Implement cleanup properly
   - Example:
   ```python
   # ✅ Good
   import pytest
   from fastmcp import FastMCP, TestClient
   from typing import AsyncGenerator
   import asyncio

   class TestEnvironment:
       def __init__(self):
           self.app = FastMCP()
           self.cache = Cache()
           self.test_client = TestClient(self.app)

       async def setup(self):
           await self.app.startup()
           await self.cache.connect()

       async def cleanup(self):
           await self.cache.disconnect()
           await self.app.shutdown()

   @pytest.fixture
   async def test_env() -> AsyncGenerator[TestEnvironment, None]:
       env = TestEnvironment()
       await env.setup()
       yield env
       await env.cleanup()

   class TestIntegration:
       @pytest.mark.asyncio
       async def test_full_request_flow(
           self,
           test_env: TestEnvironment
       ):
           # Arrange
           data = {"key": "value"}

           # Act
           response = await test_env.test_client.post(
               "/process",
               json=data
           )

           # Assert
           assert response.status_code == 200
           result = response.json()
           assert "result" in result

           # Verify cache
           cached = await test_env.cache.get(
               f"process:{data['key']}"
           )
           assert cached is not None

   # ❌ Bad
   def test_bad_integration():
       app = FastMCP()
       response = app.client.get("/")  # No setup/cleanup
       assert response.ok  # Insufficient verification
   ```

3. **Error Simulation**
   - Test error scenarios
   - Simulate system failures
   - Handle timeout conditions
   - Test recovery mechanisms
   - Example:
   ```python
   # ✅ Good
   import pytest
   from fastmcp import FastMCP
   from unittest.mock import AsyncMock
   import asyncio

   class ErrorSimulator:
       def __init__(self, component: Any):
           self._component = component
           self._original_methods = {}

       async def simulate_timeout(
           self,
           method_name: str,
           delay: float
       ):
           original = getattr(self._component, method_name)
           self._original_methods[method_name] = original

           async def delayed_method(*args, **kwargs):
               await asyncio.sleep(delay)
               return await original(*args, **kwargs)

           setattr(self._component, method_name, delayed_method)

       async def simulate_error(
           self,
           method_name: str,
           error: Exception
       ):
           original = getattr(self._component, method_name)
           self._original_methods[method_name] = original

           async def error_method(*args, **kwargs):
               raise error

           setattr(self._component, method_name, error_method)

       def restore(self):
           for name, method in self._original_methods.items():
               setattr(self._component, name, method)

   class TestErrorScenarios:
       @pytest.mark.asyncio
       async def test_timeout_handling(self):
           # Arrange
           app = FastMCP()
           simulator = ErrorSimulator(app.cache)
           await simulator.simulate_timeout("get", 2.0)

           try:
               # Act
               with pytest.raises(TimeoutError):
                   await asyncio.wait_for(
                       app.cache.get("key"),
                       timeout=1.0
                   )
           finally:
               simulator.restore()

   # ❌ Bad
   def test_bad_errors():
       try:
           raise Exception("error")  # No specific scenarios
       except:
           pass  # No verification
   ```

4. **Debugging Strategies**
   - Implement proper logging
   - Use debugging contexts
   - Handle debug information
   - Support troubleshooting
   - Example:
   ```python
   # ✅ Good
   import structlog
   from contextlib import contextmanager
   from typing import Any, Generator
   import time

   class DebugContext:
       def __init__(self, logger: Any):
           self._logger = logger
           self._start_time = None
           self._context = {}

       @contextmanager
       def operation(
           self,
           name: str,
           **kwargs: Any
       ) -> Generator[None, None, None]:
           self._start_time = time.monotonic()
           self._logger.debug(
               f"{name}_start",
               **kwargs
           )
           try:
               yield
           except Exception as e:
               self._logger.error(
                   f"{name}_error",
                   error=str(e),
                   **kwargs
               )
               raise
           finally:
               duration = time.monotonic() - self._start_time
               self._logger.debug(
                   f"{name}_end",
                   duration=duration,
                   **kwargs
               )

   class DebuggableComponent:
       def __init__(self):
           self._logger = structlog.get_logger()
           self._debug = DebugContext(self._logger)

       async def process(self, data: Any) -> Any:
           with self._debug.operation(
               "process_data",
               data_type=type(data).__name__
           ):
               result = await self._process_internal(data)
               return result

   # ❌ Bad
   class BadDebug:
       def process(self, data):
           print(f"Processing: {data}")  # Poor debugging
           return process(data)
   ```

5. **Performance Testing**
   - Measure response times
   - Test under load
   - Monitor resource usage
   - Verify scaling behavior
   - Example:
   ```python
   # ✅ Good
   import pytest
   from fastmcp import FastMCP, LoadTester
   from typing import List
   import asyncio
   import statistics

   class PerformanceTest:
       def __init__(
           self,
           app: FastMCP,
           concurrent_users: int = 10,
           duration: float = 60.0
       ):
           self._app = app
           self._users = concurrent_users
           self._duration = duration
           self._results: List[float] = []

       async def run_load_test(self) -> dict:
           start = asyncio.get_event_loop().time()
           tasks = []

           for _ in range(self._users):
               task = asyncio.create_task(
                   self._user_session()
               )
               tasks.append(task)

           await asyncio.gather(*tasks)

           return {
               "total_requests": len(self._results),
               "avg_response_time": statistics.mean(self._results),
               "p95_response_time": statistics.quantiles(
                   self._results,
                   n=20
               )[18],  # 95th percentile
               "max_response_time": max(self._results)
           }

       async def _user_session(self):
           end_time = asyncio.get_event_loop().time() + self._duration
           while asyncio.get_event_loop().time() < end_time:
               start = asyncio.get_event_loop().time()
               await self._app.test_client.get("/test")
               self._results.append(
                   asyncio.get_event_loop().time() - start
               )
               await asyncio.sleep(1.0)  # Think time

   # ❌ Bad
   def test_bad_performance():
       app = FastMCP()
       start = time.time()
       app.test_client.get("/")  # Single request
       duration = time.time() - start
       assert duration < 1  # Arbitrary threshold
   ```

## Examples

<example>
# Complete Testing Implementation
import pytest
from fastmcp import FastMCP, TestClient, Cache
from typing import AsyncGenerator, Any
import structlog
import asyncio
from datetime import timedelta

logger = structlog.get_logger()

# Test Configuration
class TestConfig(BaseModel):
    cache_url: str = "memory://"
    test_timeout: float = 5.0
    cleanup_timeout: float = 2.0

# Test Environment
class TestEnvironment:
    def __init__(self, config: TestConfig):
        self.config = config
        self.app = FastMCP()
        self.cache = Cache(config.cache_url)
        self.test_client = TestClient(self.app)
        self._debug = DebugContext(logger)

    async def setup(self):
        with self._debug.operation("test_setup"):
            await self.app.startup()
            await self.cache.connect()

    async def cleanup(self):
        with self._debug.operation("test_cleanup"):
            await self.cache.disconnect()
            await self.app.shutdown()

# Test Fixtures
@pytest.fixture
async def test_env(
    test_config: TestConfig
) -> AsyncGenerator[TestEnvironment, None]:
    env = TestEnvironment(test_config)
    await env.setup()
    yield env
    await env.cleanup()

@pytest.fixture
def error_simulator(test_env: TestEnvironment) -> ErrorSimulator:
    simulator = ErrorSimulator(test_env.cache)
    yield simulator
    simulator.restore()

# Test Cases
class TestIntegration:
    @pytest.mark.asyncio
    async def test_successful_flow(
        self,
        test_env: TestEnvironment
    ):
        # Arrange
        data = {"key": "test_value"}

        # Act
        response = await test_env.test_client.post(
            "/process",
            json=data
        )

        # Assert
        assert response.status_code == 200
        result = response.json()
        assert result["status"] == "success"

        # Verify cache
        cached = await test_env.cache.get(
            f"process:{data['key']}"
        )
        assert cached is not None

    @pytest.mark.asyncio
    async def test_error_handling(
        self,
        test_env: TestEnvironment,
        error_simulator: ErrorSimulator
    ):
        # Arrange
        await error_simulator.simulate_error(
            "get",
            Exception("Simulated failure")
        )

        # Act
        response = await test_env.test_client.get("/data")

        # Assert
        assert response.status_code == 500
        error = response.json()
        assert "error" in error

    @pytest.mark.asyncio
    async def test_performance(
        self,
        test_env: TestEnvironment
    ):
        # Arrange
        perf_test = PerformanceTest(
            test_env.app,
            concurrent_users=5,
            duration=10.0
        )

        # Act
        results = await perf_test.run_load_test()

        # Assert
        assert results["avg_response_time"] < 0.1
        assert results["p95_response_time"] < 0.2
</example>

<example type="invalid">
# Bad Testing Implementation
class BadTests:
    def __init__(self):
        self.app = FastMCP()

    def test_api(self):
        # No proper setup
        response = self.app.client.get("/")
        assert response.status_code == 200

    def test_cache(self):
        # Global state
        cache = {}
        cache["key"] = "value"
        assert cache["key"] == "value"

    def test_errors(self):
        try:
            self.app.process({})
        except:
            pass  # Silent failure

    def test_performance(self):
        start = time.time()
        self.app.client.get("/")
        duration = time.time() - start
        print(f"Duration: {duration}")  # No assertions
</example>
